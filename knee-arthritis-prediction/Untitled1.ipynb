{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e306fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyswarm in c:\\users\\shaya\\appdata\\roaming\\python\\python39\\site-packages (0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaya\\appdata\\roaming\\python\\python39\\site-packages (from pyswarm) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyswarm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pyswarm import pso\n",
    "\n",
    "# Define file paths for data\n",
    "data_dir = 'test'\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "\n",
    "# def auto_split_dataset(input_folder, output_folder):\n",
    "#     # Create output folders if not exist\n",
    "#     for folder in ['train', 'test', 'val']:\n",
    "#         os.makedirs(os.path.join(output_folder, folder, 'fire'), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(output_folder, folder, 'nofire'), exist_ok=True)\n",
    "\n",
    "#     # Get the list of image filenames\n",
    "#     fire_images = os.listdir(os.path.join(input_folder, 'fire'))\n",
    "#     nofire_images = os.listdir(os.path.join(input_folder, 'nofire'))\n",
    "\n",
    "#     # Split the dataset into training, testing, and validation sets\n",
    "#     fire_train, fire_temp = train_test_split(fire_images, test_size=0.3, random_state=42)\n",
    "#     fire_test, fire_val = train_test_split(fire_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#     nofire_train, nofire_temp = train_test_split(nofire_images, test_size=0.3, random_state=42)\n",
    "#     nofire_test, nofire_val = train_test_split(nofire_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#     # Copy images to respective folders\n",
    "#     for image in fire_train:\n",
    "#         copyfile(os.path.join(input_folder, 'fire', image), os.path.join(output_folder, 'train', 'fire', image))\n",
    "\n",
    "#     for image in fire_test:\n",
    "#         copyfile(os.path.join(input_folder, 'fire', image), os.path.join(output_folder, 'test', 'fire', image))\n",
    "\n",
    "#     for image in fire_val:\n",
    "#         copyfile(os.path.join(input_folder, 'fire', image), os.path.join(output_folder, 'val', 'fire', image))\n",
    "\n",
    "#     for image in nofire_train:\n",
    "#         copyfile(os.path.join(input_folder, 'nofire', image), os.path.join(output_folder, 'train', 'nofire', image))\n",
    "\n",
    "#     for image in nofire_test:\n",
    "#         copyfile(os.path.join(input_folder, 'nofire', image), os.path.join(output_folder, 'test', 'nofire', image))\n",
    "\n",
    "#     for image in nofire_val:\n",
    "#         copyfile(os.path.join(input_folder, 'nofire', image), os.path.join(output_folder, 'val', 'nofire', image))\n",
    "\n",
    "# # Example usage:\n",
    "# input_folder = '/content/drive/MyDrive/Dataset'\n",
    "# output_folder = '/content/drive/MyDrive/Output'\n",
    "# auto_split_dataset(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a21855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b880d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1162 images belonging to 5 classes.\n",
      "Found 494 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "classes = ['0', '1','2','3','4']\n",
    "\n",
    "# Create ImageDataGenerator for data augmentation and normalization\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.3)\n",
    "\n",
    "# Load and split the data into training, validation, and testing sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    classes=classes,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    classes=classes,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e961ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of classes dynamically\n",
    "# Create InceptionV3 model with pre-trained weights\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom head to the base model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6aff44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Hybrid PSO\n",
    "def objective_function(x):\n",
    "    # Set the hyperparameters based on the particle values\n",
    "    learning_rate = x[0]\n",
    "    num_epochs = math.ceil(x[1])\n",
    "\n",
    "    # Train the model with the updated hyperparameters\n",
    "    model.fit(train_generator, validation_data=val_generator, epochs=num_epochs, verbose=0)\n",
    "\n",
    "    # Obtain the accuracy on the validation set\n",
    "    _, val_acc = model.evaluate(val_generator)\n",
    "\n",
    "    # Return the negative accuracy to maximize it in PSO\n",
    "    return -val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ef7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Hybrid PSO parameters\n",
    "def hybrid_pso(objective_function, lower_bounds, upper_bounds, max_iterations=2, swarm_size=1):\n",
    "    inertia_weight = 0.5\n",
    "    c1 = 1.5\n",
    "    c2 = 1.5\n",
    "    bounds = list(zip(lower_bounds, upper_bounds))\n",
    "\n",
    "    # Initialize particles and velocities\n",
    "    particles = np.random.uniform(lower_bounds, upper_bounds, (swarm_size, len(lower_bounds)))\n",
    "    velocities = np.random.uniform(-1, 1, (swarm_size, len(lower_bounds)))\n",
    "\n",
    "    # Initialize personal best positions and fitness values\n",
    "    personal_best_positions = particles.copy()\n",
    "    personal_best_fitness = np.array([objective_function(p) for p in particles])\n",
    "\n",
    "    # Initialize global best position and fitness value\n",
    "    global_best_index = np.argmin(personal_best_fitness)\n",
    "    global_best_position = particles[global_best_index]\n",
    "    global_best_fitness = personal_best_fitness[global_best_index]\n",
    "\n",
    "    # Initialize memory matrix\n",
    "    memory_matrix = np.zeros_like(particles)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        for i in range(swarm_size):\n",
    "            # Update particle velocity using the Hybrid PSO rule\n",
    "            r1, r2 = np.random.rand(), np.random.rand()\n",
    "\n",
    "            cognitive_component = c1 * r1 * (personal_best_positions[i] - particles[i])\n",
    "            social_component = c2 * r2 * (global_best_position - particles[i])\n",
    "            memory_component = inertia_weight * memory_matrix[i]\n",
    "\n",
    "            velocities[i] = inertia_weight * velocities[i] + cognitive_component + social_component + memory_component\n",
    "\n",
    "            # Update particle position\n",
    "            particles[i] += velocities[i]\n",
    "\n",
    "            # Ensure particle stays within bounds\n",
    "            particles[i] = np.clip(particles[i], lower_bounds, upper_bounds)\n",
    "\n",
    "            # Evaluate fitness of the new position\n",
    "            fitness = objective_function(particles[i])\n",
    "\n",
    "            # Update personal best if needed\n",
    "            if fitness < personal_best_fitness[i]:\n",
    "                personal_best_fitness[i] = fitness\n",
    "                personal_best_positions[i] = particles[i]\n",
    "\n",
    "                # Update global best if needed\n",
    "                if fitness < global_best_fitness:\n",
    "                    global_best_fitness = fitness\n",
    "                    global_best_position = particles[i]\n",
    "\n",
    "        # Update memory matrix\n",
    "        memory_matrix = personal_best_positions.copy()\n",
    "\n",
    "        # Update inertia weight (self-adaptation)\n",
    "        inertia_weight = max(0.4, inertia_weight - 0.002)\n",
    "\n",
    "    return global_best_position, global_best_fitness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c44ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the bounds for the hyperparameters\n",
    "lower_bounds = [0.001, 2]  # Lower bounds for learning_rate and num_epochs\n",
    "upper_bounds = [0.003, 3]  # Upper bounds for learning_rate and num_epochs\n",
    "swarm_size = 2  # Adjust the number of particles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cb2f74",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[32,2] labels_size=[32,5]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits\n (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend.py:5009)\n]] [Op:__inference_train_function_18217]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node categorical_crossentropy/softmax_cross_entropy_with_logits:\nIn[0] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape:\t\nIn[1] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\953607624.py\", line 2, in <module>\n>>>     best_solution, best_fitness = hybrid_pso(objective_function, lower_bounds, upper_bounds, max_iterations=2, swarm_size=swarm_size)\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\3829828665.py\", line 14, in hybrid_pso\n>>>     personal_best_fitness = np.array([objective_function(p) for p in particles])\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\3829828665.py\", line 14, in <listcomp>\n>>>     personal_best_fitness = np.array([objective_function(p) for p in particles])\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\3459099473.py\", line 8, in objective_function\n>>>     model.fit(train_generator, validation_data=val_generator, epochs=num_epochs, verbose=0)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1664, in categorical_crossentropy\n>>>     return backend.categorical_crossentropy(\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5009, in categorical_crossentropy\n>>>     return tf.nn.softmax_cross_entropy_with_logits(\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9828\\953607624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Perform Hybrid PSO optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_solution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_fitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhybrid_pso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswarm_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswarm_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9828\\3829828665.py\u001b[0m in \u001b[0;36mhybrid_pso\u001b[1;34m(objective_function, lower_bounds, upper_bounds, max_iterations, swarm_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Initialize personal best positions and fitness values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mpersonal_best_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpersonal_best_fitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparticles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Initialize global best position and fitness value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9828\\3829828665.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Initialize personal best positions and fitness values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mpersonal_best_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpersonal_best_fitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparticles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Initialize global best position and fitness value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9828\\3459099473.py\u001b[0m in \u001b[0;36mobjective_function\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Train the model with the updated hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Obtain the accuracy on the validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[32,2] labels_size=[32,5]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits\n (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend.py:5009)\n]] [Op:__inference_train_function_18217]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node categorical_crossentropy/softmax_cross_entropy_with_logits:\nIn[0] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape:\t\nIn[1] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\953607624.py\", line 2, in <module>\n>>>     best_solution, best_fitness = hybrid_pso(objective_function, lower_bounds, upper_bounds, max_iterations=2, swarm_size=swarm_size)\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\3829828665.py\", line 14, in hybrid_pso\n>>>     personal_best_fitness = np.array([objective_function(p) for p in particles])\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\3829828665.py\", line 14, in <listcomp>\n>>>     personal_best_fitness = np.array([objective_function(p) for p in particles])\n>>> \n>>>   File \"C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_9828\\3459099473.py\", line 8, in objective_function\n>>>     model.fit(train_generator, validation_data=val_generator, epochs=num_epochs, verbose=0)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1664, in categorical_crossentropy\n>>>     return backend.categorical_crossentropy(\n>>> \n>>>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5009, in categorical_crossentropy\n>>>     return tf.nn.softmax_cross_entropy_with_logits(\n>>> "
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform Hybrid PSO optimization\n",
    "best_solution, best_fitness = hybrid_pso(objective_function, lower_bounds, upper_bounds, max_iterations=2, swarm_size=swarm_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58016ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the best solution and its fitness\n",
    "print(\"Best Solution (Learning Rate, Num Epochs):\", best_solution)\n",
    "print(\"Best Fitness (Validation Accuracy):\", -best_fitness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ec68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_num_epochs = math.ceil(best_solution[1])\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=best_num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_generator.reset()\n",
    "val_pred_labels = np.argmax(model.predict(val_generator), axis=1)\n",
    "val_true_labels = val_generator.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5267520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate classification report and confusion matrix\n",
    "report = classification_report(val_true_labels, val_pred_labels, target_names=classes)\n",
    "confusion_mat = confusion_matrix(val_true_labels, val_pred_labels)\n",
    "\n",
    "# Plot training and validation confusion matrices with heatmaps\n",
    "plt.imshow(confusion_mat, cmap='Blues')\n",
    "plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Validation Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Calculate Precision, Recall, F1-Score\n",
    "precision = precision_score(val_true_labels, val_pred_labels)\n",
    "recall = recall_score(val_true_labels, val_pred_labels)\n",
    "f1 = f1_score(val_true_labels, val_pred_labels)\n",
    "\n",
    "# Calculate True Positives, False Positives, True Negatives, False Negatives\n",
    "tp = confusion_mat[1, 1]\n",
    "fp = confusion_mat[0, 1]\n",
    "tn = confusion_mat[0, 0]\n",
    "fn = confusion_mat[1, 0]\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Negatives:\", fn)\n",
    "\n",
    "# Plot the training and validation accuracy graph\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss graph\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_generator.reset()\n",
    "train_pred_labels = np.argmax(model.predict(train_generator), axis=1)\n",
    "train_true_labels = train_generator.classes\n",
    "\n",
    "# Calculate confusion matrix for the training set\n",
    "train_confusion_mat = confusion_matrix(train_true_labels, train_pred_labels)\n",
    "\n",
    "# Plot training confusion matrix\n",
    "plt.imshow(train_confusion_mat, cmap='Blues')\n",
    "plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Training Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot validation confusion matrix\n",
    "plt.imshow(confusion_mat, cmap='Blues')\n",
    "plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Validation Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the number of images per class in the training set\n",
    "train_class_counts = np.sum(train_generator.labels == np.arange(len(classes))[:, None], axis=1)\n",
    "\n",
    "# Calculate the number of images per class in the validation set\n",
    "val_class_counts = np.sum(val_generator.labels == np.arange(len(classes))[:, None], axis=1)\n",
    "\n",
    "# Create a line plot for the number of images per class\n",
    "plt.plot(classes, train_class_counts, label='Training Set', marker='o')\n",
    "plt.plot(classes, val_class_counts, label='Validation Set', marker='o')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.legend()\n",
    "plt.title('Number of Images per Class')\n",
    "plt.show()\n",
    "\n",
    "# Assuming you have 2 classes, get probabilities for the positive class (fire)\n",
    "val_pred_probs_fire = val_pred_probs[:, 1]\n",
    "\n",
    "# Calculate ROC curves and AUC for each class\n",
    "fpr, tpr, thresholds = roc_curve(val_true_labels, val_pred_probs_fire)\n",
    "roc_auc = roc_auc_score(val_true_labels, val_pred_probs_fire)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.title('ROC Curve for Fire Detection')\n",
    "plt.show()\n",
    "\n",
    "# Calculate error rates for each class\n",
    "threshold = 0.5  # Adjust the threshold based on your requirements\n",
    "val_pred_labels = (val_pred_probs_fire > threshold).astype(int)\n",
    "error_rates = np.mean(val_pred_labels != val_true_labels, axis=0)\n",
    "\n",
    "# Ensure error_rates is a 1D array\n",
    "error_rates = np.squeeze(error_rates)\n",
    "\n",
    "# Print error rates for each class\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"Error Rate for {class_name}: {error_rates:.4f}\")\n",
    "\n",
    "# Calculate the number of images per class in the training set\n",
    "train_class_counts = np.sum(train_generator.labels == np.arange(len(classes))[:, None], axis=1)\n",
    "\n",
    "# Calculate the number of images per class in the validation set\n",
    "val_class_counts = np.sum(val_generator.labels == np.arange(len(classes))[:, None], axis=1)\n",
    "\n",
    "# Create a bar plot for the number of images per class\n",
    "plt.bar(classes, train_class_counts, label='Training Set', alpha=0.7)\n",
    "plt.bar(classes, val_class_counts, label='Validation Set', alpha=0.7)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.legend()\n",
    "plt.title('Number of Images per Class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
